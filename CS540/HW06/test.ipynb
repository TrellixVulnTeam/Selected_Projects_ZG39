{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.linear import Linear\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(training=True):\n",
    "    \"\"\"\n",
    "    TODO: implement this function.\n",
    "\n",
    "    INPUT: \n",
    "        An optional boolean argument (default value is True for training dataset)\n",
    "\n",
    "    RETURNS:\n",
    "        Dataloader for the training set (if training = True) or the test set (if training = False)\n",
    "    \"\"\"\n",
    "    custom_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.MNIST('./data', train = True, download = True,\n",
    "                               transform = custom_transform)\n",
    "\n",
    "    test_set = datasets.MNIST('./data', train = False,\n",
    "                              transform = custom_transform)\n",
    "\n",
    "    if training == True:\n",
    "        # train loader\n",
    "        return torch.utils.data.DataLoader(train_set, batch_size = 50)\n",
    "    else:\n",
    "        # test loader\n",
    "        return torch.utils.data.DataLoader(test_set, batch_size = 50, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:00, 11582271.42it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 29431486.67it/s]         \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 11231871.58it/s]                           \n",
      "5120it [00:00, ?it/s]                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_data_loader()\n",
    "print(type(train_loader))\n",
    "print(train_loader.dataset)\n",
    "test_loader = get_data_loader(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    TODO: implement this function.\n",
    "\n",
    "    INPUT: \n",
    "        None\n",
    "\n",
    "    RETURNS:\n",
    "        An untrained neural network model\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        # flatten layer\n",
    "        nn.Flatten(),\n",
    "        # dense layer\n",
    "        nn.Linear(784, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 10)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "def train_model(model, train_loader, criterion, T):\n",
    "    \"\"\"\n",
    "    TODO: implement this function.\n",
    "\n",
    "    INPUT: \n",
    "        model - the model produced by the previous function\n",
    "        train_loader  - the train DataLoader produced by the first function\n",
    "        criterion   - cross-entropy \n",
    "        T - number of epochs for training\n",
    "\n",
    "    RETURNS:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # set model to train model before iterating\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(T):  # outer for loop iterates over epochs\n",
    "        # initialize loss & accuracy\n",
    "        average_loss = 0.0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # inner for loop iterates over batches of (iamges, labels) pairs from\n",
    "        # the train DataLoader\n",
    "        for i, datas in enumerate(train_loader, 0):\n",
    "            # get the inputs; data is a list of [images, labels]\n",
    "            images, labels = datas\n",
    "\n",
    "            # zero param gradients\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            # calculate outputs by running images through nn\n",
    "            outputs = model(images)\n",
    "            # class with highest energy is choosed for prediction\n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # write statistics into print variables\n",
    "            average_loss += loss.item()\n",
    "            accuracy += (predict == labels).sum().item()\n",
    "\n",
    "        # print statistics\n",
    "        print(\"Train Epoch: %d Accuracy: %d/60000(%.2f%%) Loss: %.3f\" %\n",
    "                (epoch, accuracy, accuracy * 100 / 60000, average_loss *50 / 60000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Accuracy: 47327/60000(78.88%) Loss: 0.770\n",
      "Train Epoch: 1 Accuracy: 54812/60000(91.35%) Loss: 0.295\n",
      "Train Epoch: 2 Accuracy: 55865/60000(93.11%) Loss: 0.236\n",
      "Train Epoch: 3 Accuracy: 56538/60000(94.23%) Loss: 0.197\n",
      "Train Epoch: 4 Accuracy: 57047/60000(95.08%) Loss: 0.169\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "train_model(model, train_loader, criterion, T=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, show_loss=True):\n",
    "    \"\"\"\n",
    "    TODO: implement this function.\n",
    "\n",
    "    INPUT: \n",
    "        model - the the trained model produced by the previous function\n",
    "        test_loader    - the test DataLoader\n",
    "        criterion   - cropy-entropy \n",
    "\n",
    "    RETURNS:\n",
    "        None\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    accuracy = 0\n",
    "    average_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # disable gradient tracking during testing\n",
    "    with torch.no_grad():\n",
    "        for datas in test_loader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            images, labels = datas\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            # calculate outputs by running images through nn\n",
    "            outputs = model(images)\n",
    "            # class with highest energy is choosed for prediction\n",
    "            _, predict = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # write statistics into print variables\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predict == labels).sum().item()\n",
    "            average_loss += loss.item()\n",
    "            \n",
    "\n",
    "    # print test Loss & test Accuracy\n",
    "    if show_loss == True:\n",
    "        print(\"Average loss: %.4f\\nAccuracy: %.2f%%\" %\n",
    "              (average_loss / total, accuracy * 100 / total))\n",
    "    else:\n",
    "        print(\"Accuracy: %.2f%%\" % (accuracy * 100 / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.34%\n",
      "Average loss: 0.0031\n",
      "Accuracy: 95.34%\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, criterion, show_loss=False)\n",
    "evaluate_model(model, test_loader, criterion, show_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(model, test_images, index):\n",
    "    \"\"\"\n",
    "    TODO: implement this function.\n",
    "\n",
    "    INPUT: \n",
    "        model - the trained model\n",
    "        test_images   -  test image set of shape Nx1x28x28\n",
    "        index   -  specific index  i of the image to be tested: 0 <= i <= N - 1\n",
    "\n",
    "\n",
    "    RETURNS:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    class_names = ['zero', 'one', 'two', 'three',\n",
    "                   'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "    logits = model(test_images)\n",
    "    prob = F.softmax(logits, dim = 1)\n",
    "    list_prob = []\n",
    "\n",
    "    # add probability into list_prob \n",
    "    for i in range (0, 10):\n",
    "        list_prob.append((class_names[i], prob[index][i].item() * 100))\n",
    "    \n",
    "    # rank in descending order of probability\n",
    "    list_prob.sort(key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # print out the first 3 greatest probability\n",
    "    for i in range (0, 3):\n",
    "        print(\"%s: %.2f%%\" % (list_prob[i][0], list_prob[i][1]))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero: 99.96%\n",
      "two: 0.02%\n",
      "five: 0.01%\n"
     ]
    }
   ],
   "source": [
    "pred_set, _ = iter(get_data_loader()).next()\n",
    "predict_label(model, pred_set, 1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
